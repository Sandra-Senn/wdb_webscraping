# wdb_webscraping

Dieses Projekt automatisiert das Scrapen von Artikeln auf der Website der SRF (Schweizer Radio und Fernsehen), basierend auf einer vordefinierten Liste von Nachnamen. Die Daten werden gespeichert, analysiert und aufbereitet, um Erkenntnisse Ã¼ber Namensverteilungen und Inhalte zu gewinnen.  

## Projektstruktur

```bash
WDB_WEBSCRAPING/
â”œâ”€â”€ .github/workflows/pytest.yml                             
â”œâ”€â”€ doc/
â”‚   â””â”€â”€ Bewertungskriterien.xlsx     
â”œâ”€â”€ log/
â”‚   â””â”€â”€ scraping.log                 
â”œâ”€â”€ notebook/
â”‚   â”œâ”€â”€ eda.ipynb                    
â”‚   â””â”€â”€ Web_scraper_SRF.ipynb        
â”œâ”€â”€ output/
â”‚   â””â”€â”€ df_namen.csv           
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py                
â”‚   â””â”€â”€ utils.py               
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_scraper.py  
â”œâ”€â”€ .gitignore
â”œâ”€â”€ environment.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸ“ `.github/workflows/`
EnthÃ¤lt Konfiguration fÃ¼r **GitHub Actions** zur Automatisierung von Tests.

- `pytest.yml`: FÃ¼hrt automatisiert `pytest` bei jedem Push/PR aus.

---

### ğŸ“ `doc/`
Dokumentation des Projekts.

- `Bewertungskriterien.xlsx`: Bewertungsraster oder Aufgabenstellung.

---

### ğŸ“ `log/`
Speichert Laufzeitprotokolle fÃ¼r Debugging und Nachvollziehbarkeit.

- `scraping.log`: Logfile mit Infos zu Scraping-DurchlÃ¤ufen, Fehlern etc.

---

### ğŸ“ `notebook/`
Jupyter Notebooks fÃ¼r Analyse, Experimente und interaktive Nutzung.

- `eda.ipynb`: Explorative Datenanalyse der gesammelten Artikel.
- `Web_scraper_SRF.ipynb`: Scraping-Notebook mit den folgenden Nachnamen:
    - Rohrer
    - Meier
    - Senn
    - Schweizer
    - Walser
    - Luder
    - Torres
    - Schatzmann
    - Schmidt
    - MÃ¼ller
    - Schmid
    - Winter

---

### ğŸ“ `output/`
Alle vom Scraper erzeugten Daten.

- `df_namen.csv`: Ergebnis-Datei mit gesammelten Artikeln (Suchbegriff, Titel, Datum, Autor, Kategorie, Unterkategorie).

---

### ğŸ“ `src/`
Der zentrale Quellcode des Projekts.

- `__init__.py`: Macht den Ordner zu einem Python-Paket, um Module zu importieren.
- `scraper.py`: Hauptskript fÃ¼r das Scraping mit Selenium.
- `utils.py`: Hilfsfunktionen.

---

### ğŸ“ `tests/`
Testskripte, geschrieben mit **pytest**.

- `test_scraper.py`: Unit Tests fÃ¼r `scraper.py` und unterstÃ¼tzende Funktionen.

---

### ğŸ“„ `.gitignore`
Ignoriert `.venv/`, `.pkl`, `.log`, `.ipynb_checkpoints/` etc., um das Repo sauber zu halten.

---

### ğŸ“„ `environment.yml`
Conda-Umgebung zur einfachen Reproduzierbarkeit (Alternative zu `requirements.txt`).

---

### ğŸ“„ `requirements.txt`
Listet alle Python-AbhÃ¤ngigkeiten fÃ¼r Installation via `pip`.


## Getting Started

FÃ¼r die Insallation der benÃ¶tigten Pakete, fÃ¼hren Sie den folgenden Befehl in Ihrem Terminal aus:

```bash
pip install -r requirements.txt
```
FÃ¼r conda:

```bash
conda install --file environment.txt
```

